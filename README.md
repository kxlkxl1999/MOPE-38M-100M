# MOPE: Mixture of Optimal Pruned Experts

ç®—æ³•éªŒè¯å‚è€ƒæ¨¡å‹ï¼šFLAME-MoE-115M-459M

## FLAME-MoE :fire:â€‹: A Transparent End-to-End Research Platform for Mixture-of-Experts Language Models

**FLAME-MoE** is a transparent, end-to-end research platform for Mixture-of-Experts (MoE) language models. It is designed to facilitate scalable training, evaluation, and experimentation with MoE architectures. [arXiv](https://www.arxiv.org/abs/2505.20225)

### ğŸ”— Model Checkpoints

Explore our publicly released checkpoints on Hugging Face:

* [FLAME-MoE-1.7B-10.3B](https://huggingface.co/CMU-FLAME/FLAME-MoE-1.7B-10.3B)
* [FLAME-MoE-721M-3.8B](https://huggingface.co/CMU-FLAME/FLAME-MoE-721M-3.8B)
* [FLAME-MoE-419M-2.2B](https://huggingface.co/CMU-FLAME/FLAME-MoE-419M-2.2B)
* [FLAME-MoE-290M-1.3B](https://huggingface.co/CMU-FLAME/FLAME-MoE-290M-1.3B)
* [FLAME-MoE-115M-459M](https://huggingface.co/CMU-FLAME/FLAME-MoE-115M-459M)
* [FLAME-MoE-98M-349M](https://huggingface.co/CMU-FLAME/FLAME-MoE-98M-349M)
* [FLAME-MoE-38M-100M](https://huggingface.co/CMU-FLAME/FLAME-MoE-38M-100M)

---

### ğŸš€ Getting Started

#### 1. Clone the Repository

Ensure you clone the repository **recursively** to include all submodules:

```bash
git clone --recursive https://github.com/cmu-flame/MoE-Research
cd MoE-Research
```
åŒæ—¶ä¸‹è½½ clone apex, Megatron-LM, TransformerEngine åˆ°å¯¹åº”æ–‡ä»¶å¤¹ã€‚

#### 2. Set Up the Environment

Set up the Conda environment using the provided script:

å®‰è£…å¥½cuda12.4 https://developer.nvidia.com/cuda-12-4-1-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=runfile_local
å®˜ç½‘å®‰è£…cudnn https://developer.nvidia.com/cudnn-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_network

```bash
sudo apt-get install build-essential.
# scripts/miscellaneous/install.shé‡Œçš„å®‰è£…åˆ†å¼€ä¸€æ­¥æ­¥æ‰§è¡Œå°±å¥½
# sbatch scripts/miscellaneous/install.sh
```


> **Note:** This assumes you're using a SLURM-managed cluster. Adapt accordingly if running locally.

---

### ğŸ“š Data Preparation

#### 3. Download and Tokenize the Dataset

ä»æ­¤ä¸‹è½½æ•°æ®ï¼šhttps://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0/tree/main/global-shard_03_of_10/local-shard_1_of_10
åˆ‡è¯ä½¿ç”¨
```bash
bash local_tokenize_pythia.sh
```

---

### ğŸ§  Training

#### 4. Train FLAME-MoE Models

ä¸‹è½½weight
```python
from huggingface_hub import snapshot_download

snapshot_download(
    repo_id="CMU-FLAME/FLAME-MoE-38M-100M",
    repo_type="model",
    local_dir="./weight_initial",  # æœ¬åœ°ä¿å­˜è·¯å¾„
    local_dir_use_symlinks=False  # é¿å…è½¯é“¾æ¥ï¼Œç¡®ä¿æ˜¯å®Œæ•´æ–‡ä»¶
)
```

```bash
bash scripts/release/mymoe-38m-froze.sh
```

---

### ğŸ“ˆ Evaluation

#### 5. Evaluate the Model

To evaluate a trained model, set the appropriate job ID and iteration number before submitting the evaluation script:

```bash
#export JOBID=...    # Replace with your training job ID
#export ITER=...     # Replace with the iteration to evaluate (e.g., 11029)
sbatch scripts/myeval.sh
```
